---
title: "Mar 13 2018 Notes"
date: 2018-03-13T08:47:49-04:00
draft: false
tags: ["18.175"]
---

# 18.175

Started PSET. Notes from prerequisites of the pset.

Caratheodory extension theorem: If \\(\mu\\) is \\(\sigma\\)-finite on \\(\mathcal{A}\\),
then \\(\mu\\) has an unique extension to the \\(\sigma\\)-algebra generated by \\(\mathcal{A}\\).

Use this to prove uniqueness of translation invariant measure giving one to the unit cube.
That is, the lebesgue measure.

Dilemma: We are lazy. We want sigma algebras to be big because that will give us as much power as 
possible, but we don't want to define everything. Measure theory is tedius as it stands, not
need to make it worse. So let's "cheat"! That is, use extension theorems that will let us
focus on the nice parts and automatically do the rest.


Extension on semi algebras (closed under intersection, and compliment generated by unions):
If \\(\mu\\) is defined on a semi algebra, the null set is given zero, and is finitely additive
and countably subadditive, then there exist an unique extension \\(\overline{\mu}\\) 
that is a measure on the algebra generated by the semi algebra. If \\(\overline{\mu}\\)
is sigma finite, then there's one on the sigma algebra generated by the algebra (generated by the
semi algebra).

In short, for nice enough things we will only define them for semi algrebras. For \\(\mathbf{R}^k\\)
, this is stuff like \\(\prod_i (-a_i,b_i]\\).

For any function \\(F\\) that is continuous, nondecreasing, and tending to \\(0\\) to the left 
and \\(1\\) to the right, there's an unique measure defined on \\(P((a,b]) = F(b) - F(a)\\).

To prove \\(f: (\Omega,\mathcal{F})\to (S, \mathcal{S})\\) is measurable, just need to prove
the inverse of a generating set of \\(\mathcal{S}\\) is in \\(\mathcal{F}\\).

It's weird to use random variable as measurable functions. On the other hand, it shouldn't be
that weird, because of using functions to zero-one as (sub)sets. But the feeling is very different.

If it's measurable, it's integrable. Note that this implies finite measure. For integrable,
we have to check finiteness.

We are going to use the standard machine.

For expectation, this is just an intergral. Which means properties about integrals translate.
And sometimes even look nicer!

Jensen: \\(E[\phi(X)]\leq \phi(E[X])\\), for convex \\(\phi\\).

Bounded convergence: Probability measure. Suppose \\(|f_n| \leq M\\). If \\(f_n\to f\\) 
in probability, then the integrals converge.

Fatou: lim inf of integral geq integral of lim inf

Change of variables: Is allowed!

Law of large numbers: Wack. What even is i.i.d.? What even is the sigma algebra? What is happening?
What's strong? What's weak? 

(I mean, we could make sense of this statement if we created a sigma algebra of sequences or of
functions. Which probably would be in the same way of doing product in the obvious but not 
mooost obvious way like in topology. Or look at category theory! Probably category of measurable
spaces or the subcategory of probability spaces, but tbh I don't know if theres' such a thing)

Independence events if typical definition. Sets of events/sigma-algebras if pairwise independent.
Random varibles (aka measurable functions) if sigma created by inverse independent.

Another extension theorem: \\(\mathcal{A}_i\\) independent and \\(\pi\\)-systems imply
\\(\sigma(\mathcal{A}_i)\\) independent.

And indeed! for the sequence, we do a "smallest" thing, aka lowest, aka will have universal prop.
Why is this the obvious thing? Because category theory!

Kolmorogov extension theorem: If you have consistent measures on \\(\mathbf{R}^n\\),
you can extend them to \\(\mathbf{R}^{\mathbf{N}}\\)

Moments: \\(E[X^n]\\)

Moment generating function: \\(M(t) = E[e^{tX}] = \sum_n E[\frac{t^n X^n}{n!}]\\)
The \\(n\\)-th derivative is the \\(n\\)-th moment.

To be evaluable, we need things to decay superexponentially. Fat tails don't have generating 
functions, or nice ones at least.

Markov: \\(P[X\geq a] \leq \frac{E[X]}{a}\\) (or \\(aP[a\leq X] \leq E[X]\\)
(if expectation small, then it's probably not going to be big)


Chebychev:
$$ P(|X - \mu| \geq k) \leq \frac{\sigma^2}{k^2} $$
(if variance is small, it's probably be close to mean)

Weak law of large numbers: Converges in distribution (or probability?)
Strong law: Converges almost surely

Levy: \\(\Phi\_{X_n} \to \Phi_X\\)  pointwise   , then \\(X_n\to X\\) in distribution

First Borel Cantelli lemma: If \\(\sum\_{i= 0}^\infty P(A_i)  < \infty \\) then 
\\(P(A_i i.o.) = 0\\). That is, \\(P(\limsup A_i) = 0 \\). Here \\(\limsup A_i =
\bigcap\_{n = 1}^\infty \bigcup\_{i\geq n} A_i\\). That is, infinitely often.
Recall that \\(\liminf\\) would be eventually always.

Second Borel Cantelli lemma: If \\(A_n\\) independent, then \\(\sum_i P(A_i) = \infty \\)
implies \\(P(A_i i.o) = P(\limsup A_i ) = 1\\). That is, the converse.

Theorem: We have that \\(X_n\to X\\) in probability if and only if there's a subsequence 
converging almost surely. Idea: Consider events of bad outcomes, apply Borel Cantelli,
get the set of infinite often. Recurse. 

Theorem: If \\(A_n\\) independent and their probabilities sum to infinity, then if 
\\(S_n = \sum\_{i=1}^n 1\_{A_n}\\), we have that the random variable \\(\frac{S_n}{ES_n}\\)
tends to \\(1\\) almost surely. 
Independence, ratios add, then by chebychev prove convergence in probability. Take subsequence 
that converges almost surely, lift it to convergence of the whole sequence.

Strong law: Convergence almost surely. Easy to prove when kurtosis is finite, but not necessary.
WLOG zero mean. Key is to bound kurtosis of running averages.

(Dilemma: TO work with sigma algebras or with random variables?)

Kolmorogov zero-one law: You have sigma algebras/random variables \\(X_i\\). Consider 
\\(\mathcal{F}_n = \sigma(X_n, X\_{n+1},\ldots)\\). Define \\(\mathcal{T} = \bigcap_n 
\mathcal{F}_n\\). This is the tail sigma algebra. The information is about far into
the future. Intuitively, it deosn't change with finite changes of \\(X_i\\).

These are things like convergence.

Theorem: If \\(X_1,X_2,\ldots\\) are independent, and \\(A\in \mathcal{T}\\),
then \\(P(A) \in \\{0,1\\}\\).

Proof: Note that this is equivalent to saying that \\(P(A) = P(A\cap A) = P(A)^2\\). That is,
it is independent of itself. (Whack!?!?) 
We have that \\(\mathcal{A_i}\\) independent \\(\pi\\)-systems imply \\(\sigma\mathcal{A}_i\\)
independent.

We can deduce that \\(\sigma(X_1,\ldots, X_n)\\) and \\(\sigma(X\_{n+1},\ldots)\\) are independent.
We have that \\(\bigcup_k \sigma(X_1,\ldots, X_k)\\) and \\(\mathcal{T}\\) are \\(\pi\\)-systems.
Hence \\(\sigma(X_1, X_2,\ldots)\\) and \\(\mathcal{T}\\) are independent.

Hence \\(\mathcal{T}\\) is independent from \\(\mathcal{T}\\). We conclude what was desired.

Kolmorogov max inequality: Consider sequence indepndent mean zero, and running sum of that.
Probability maximum above running sum until part is inverse square of that something
times expectation of running sum.

Three series: Consider \\(X_i\\) independent and \\(A> 0\\). Consider \\(Y_i = X_i 
1\_{|X_i|\leq A}\\). We have ethat \\(X_i \to X\\) almost surely if and only if 
probability of above \\(A\\) is finite, sum of expectations of \\(Y_i\\) converge,
sum of variance of \\(Y_i\\) converges

Proof: Zero-One law implies it converges with probability zero or one. We just need to show the
probability is one if all and zero if one fails. Follow from Borel Cantelli, kolmorogov maximal.

DeMoivre-Laplace: \\(X_i\sim B(p)\\), then

$$\lim_n P\left(a\leq\frac{S_n - np}{\sqrt{npq}} \leq b\right) = P\left(a\leq Z\leq b\right)$$

where \\(Z\\) is the standard normal.
Proof: Binomial and stirling.

To be honest, thinking about the types of the integrals reveals insight about type theory. Like,
mixed embeddings are just better than shallow or deep. Or more useful. Or something.





